\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{CTEX}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{float}
\usepackage{csquotes}
\usepackage{longtable}
\usepackage[scr=rsfs]{mathalpha}
\usepackage{xcolor} 
\usepackage[colorlinks=true, allcolors=blue, urlcolor=red]{hyperref}
\renewcommand{\thefootnote}{\textcolor{red}{\arabic{footnote}}} % Change footnote number color to red
%\usepackage[hyperref=true,backend=biber,style=alpha,natbib=true,maxbibnames=99,backref=true]{biblatex}
%\addbibresource{sample.bib}
\usepackage{framed}

\title{Policy Iteration on Markov Decision Process}
\author{Bingran Li 122020087}

\begin{document}
\maketitle

\section{Introduction}

\section{Background}
Four open questions:
\begin{enumerate}
    \item Is the policy iteration method strongly polynomial for the deterministic MDP?
    \item Is there a polynomial time method for MGP (logarithmic dependence of $1/(1-\gamma)$, using IPM by the Leader)?
    \item Is there a strongly-polynomial time method for the deterministic MGP (independent of $\gamma$, extension from PY 16)?
    \item Is there a strongly polynomial-time algorithm for MDP regardless of the discount factor?
\end{enumerate}

An infinite-horizon discounted MDP can be formulated as a Linear Program D'Epenoux, 1963.

\begin{equation}
\begin{aligned}
    \max & \quad \sum_{j\in\Omega_S}v_j \\
    \text{s.t.} & \quad v_j \leq c_i^k + \gamma \sum_{j\in\Omega_S}p_{ij}^k v_j, \quad \forall i\in\Omega_S, \forall k\in\Omega_A
\end{aligned}
\end{equation}

\subsection{Complexity Results for MDPs}
Table \ref{tab:mdp_complexity} summarizes the complexity results for various types of Markov Decision Processes.

\begin{longtable}{|p{3.5cm}|p{3cm}|p{4.5cm}|p{2.5cm}|}
\caption{Complexity Results for Markov Decision Processes}
\label{tab:mdp_complexity}\\
\hline
\textbf{Problem Type} & \textbf{Algorithm} & \textbf{Complexity} & \textbf{Reference} \\
\hline
\endfirsthead

\multicolumn{4}{c}{\tablename\ \thetable{} -- continued from previous page} \\
\hline
\textbf{Problem Type} & \textbf{Algorithm} & \textbf{Complexity} & \textbf{Reference} \\
\hline
\endhead

\hline \multicolumn{4}{r}{\small Continued on next page} \\
\endfoot

\hline
\multicolumn{4}{l}{\small Note: $n$ = number of states, $m$ = number of actions, $\gamma$ = discount factor, $T$ = time horizon}\\
\endlastfoot

Discounted (general/stochastic) MDP & Simplex (most-negative-reduced-cost rule) & $O\left(\frac{mn}{1 - \gamma } \log\frac{n}{1 - \gamma}\right)$ iterations & \cite{ye2011simplex} \\
\hline
Discounted MDP & Policy Iteration, Value Iteration & $O\left(\frac{m}{1-\gamma}\log\frac{n}{1-\gamma}\right)$ iterations & \cite{hansen2013strategy} \\
\hline
Two-player turn-based stochastic games & Strategy Iteration & $O\left(\frac{m}{1-\gamma}\log\frac{n}{1-\gamma}\right)$ iterations & \cite{hansen2013strategy} \\
\hline
Deterministic MDP (uniform discount) & Simplex (highest-gain pivot rule) & $O(n^3m^2\log^2 n)$ iterations & \cite{post2015simplex} \\
\hline
Deterministic MDP (nonuniform discounts) & Simplex (highest-gain pivot rule) & $O(n^5m^3\log^2 n)$ iterations & \cite{post2015simplex} \\
\hline
Deterministic MDP (uniform discount) & Minimum Mean Cycle Algorithm & $O(mn)$ time & \cite{madani2010discounted} \\
\hline
General MDP & Specialized Interior-Point Method & Strongly polynomial in all parameters except discount factor & \cite{ye2005new} \\
\hline
General MDP & Policy Iteration & Exponential lower bound & \cite{fearnley2010exponential} \\
\hline
General MDP & Randomized simplex pivoting rules & Sub-exponential lower bound & \cite{friedmann2011subexponential} \\
\end{longtable}



\section{Summary and Discussion}

%\printbibliography
\bibliographystyle{alpha}
\bibliography{sample}
\end{document}